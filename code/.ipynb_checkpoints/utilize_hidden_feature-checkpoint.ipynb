{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from model import EncDecNet, VGG16, RNN_ENCODER\n",
    "from main import get_imgs\n",
    "\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f8d591de6d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#path = '%s/%s' %  (data_dir, 'CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#path = '/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2548\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2549\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = '/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg'\n",
    "#path = '%s/%s' %  (data_dir, 'CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg ')\n",
    "#path = '/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg '\n",
    "img = Image.open(path).convert('RGB')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/unagi0/ktokitake/encdecmodel/birds'\n",
    "pkl_path = os.path.join(data_dir, 'captions.pickle')\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    x = pkl.load(f)\n",
    "ixtoword, wordtoix = x[2], x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = [64, 128, 256]\n",
    "image_transform = transforms.Compose([\n",
    "        transforms.Scale(int(imsize[-1] * 76 / 64))])          \n",
    "norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "netG_path = 'birds_EncDecModel_2021_01_06_22_17_09'\n",
    "if os.path.exists('/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg'):\n",
    "    print('yes')\n",
    "else:\n",
    "    print('no')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def get_imgs(img_path, imsize, flip, x, y, bbox=None,\n",
    "             transform=None, normalize=None):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        r = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - r)\n",
    "        y2 = np.minimum(height, center_y + r)\n",
    "        x1 = np.maximum(0, center_x - r)\n",
    "        x2 = np.minimum(width, center_x + r)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "\n",
    "    if transform is not None:\n",
    "        img = transform(img)\n",
    "        ## crop\n",
    "        img = img.crop([x, y, x + 256, y + 256])\n",
    "        if flip:\n",
    "            img = F.hflip(img)\n",
    "\n",
    "    ret = []\n",
    "    for i in range(3):\n",
    "        if i < (3 - 1):\n",
    "            re_img = transforms.Scale(imsize[i])(img)\n",
    "        else:\n",
    "            re_img = img\n",
    "        ret.append(normalize(re_img))\n",
    "\n",
    "    return ret\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load text data\n",
      "load from img\n",
      "load to img\n"
     ]
    }
   ],
   "source": [
    "### prepare data\n",
    "text1 = 'a very colorful bird with a blue head, yellow back and orange belly.'\n",
    "text2 = ''\n",
    "sentences = [text1] ## sentences は一つであることを仮定\n",
    "captions = []\n",
    "cap_lens = []\n",
    "for sent in sentences:\n",
    "    if len(sent) == 0:\n",
    "        continue\n",
    "    sent = sent.replace(\"\\ufffd\\ufffd\", \" \")\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(sent.lower())\n",
    "    if len(tokens) == 0:\n",
    "        print('sent', sent)\n",
    "        continue\n",
    "\n",
    "    rev = []\n",
    "    for t in tokens:\n",
    "        t = t.encode('ascii', 'ignore').decode('ascii')\n",
    "        if len(t) > 0 and t in wordtoix:\n",
    "            rev.append(wordtoix[t])\n",
    "    captions.append(rev)\n",
    "    cap_lens.append(len(rev))\n",
    "\n",
    "max_len = cap_lens[0]\n",
    "cap_array = np.zeros((len(captions), max_len), dtype='int64')\n",
    "cap_array[0, :max_len] = captions[0]\n",
    "cap_lens = np.asarray(cap_lens)\n",
    "print('load text data')\n",
    "\n",
    "\n",
    "### from_img\n",
    "from_img_name = 'CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg '\n",
    "img_path = '/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/020.Yellow_breasted_Chat/Yellow_Breasted_Chat_0023_21664.jpg'\n",
    "flip = random.rand() > 0.5\n",
    "new_w = new_h = int(256 * 76 / 64)\n",
    "x = random.randint(0, np.maximum(0, new_w - 256))\n",
    "y = random.randint(0, np.maximum(0, new_h - 256))\n",
    "#img_path = '%s/%s' % (data_dir, from_img_name)\n",
    "#img_path = os.path.join(data_dir, from_img_name)\n",
    "from_imgs = get_imgs(img_path, imsize, flip, x, y,\n",
    "                None, image_transform, norm)\n",
    "print('load from img')\n",
    "\n",
    "\n",
    "### to_img\n",
    "to_img_name = 'CUB_200_2011/images/015.Lazuli_Bunting/Lazuli_Bunting_0020_14837.jpg '\n",
    "flip = random.rand() > 0.5\n",
    "new_w = new_h = int(256 * 76 / 64)\n",
    "x = random.randint(0, np.maximum(0, new_w - 256))\n",
    "y = random.randint(0, np.maximum(0, new_h - 256))\n",
    "#img_path = '%s/%s' % (data_dir, to_img_name)\n",
    "img_path ='/data/unagi0/ktokitake/encdecmodel/birds/CUB_200_2011/images/015.Lazuli_Bunting/Lazuli_Bunting_0020_14837.jpg'\n",
    "\n",
    "\n",
    "to_imgs = get_imgs(img_path, imsize, flip, x, y,\n",
    "                None, image_transform, norm)\n",
    "\n",
    "print('load to img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load text encoder from: ../DAMSMencoders/bird/text_encoder.pth\n",
      "Load the VGG model\n",
      "Load G from:  /data/unagi0/ktokitake/encdecmodel/birds/output/birds_EncDecModel_2021_01_06_22_17_09/Model/netG_epoch_600.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EncDecNet(\n",
       "  (canet): CA_NET(\n",
       "    (fc): Linear(in_features=256, out_features=400, bias=True)\n",
       "    (relu): GLU()\n",
       "  )\n",
       "  (dec_net): DecNet(\n",
       "    (upsample1): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): GLU()\n",
       "    )\n",
       "    (upsample2): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): GLU()\n",
       "    )\n",
       "    (upsample3): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): GLU()\n",
       "    )\n",
       "    (upsample4): Sequential(\n",
       "      (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "      (1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): GLU()\n",
       "    )\n",
       "    (sent_dec1): SentDec1(\n",
       "      (fc): Sequential(\n",
       "        (0): Linear(in_features=200, out_features=4096, bias=True)\n",
       "        (1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): GLU()\n",
       "      )\n",
       "      (upsample1): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(128, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): GLU()\n",
       "      )\n",
       "      (upsample2): Sequential(\n",
       "        (0): Upsample(scale_factor=2.0, mode=nearest)\n",
       "        (1): Conv2d(256, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): GLU()\n",
       "      )\n",
       "      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (sigmoid): Sigmoid()\n",
       "    )\n",
       "    (residual1): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): GLU()\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): GLU()\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (residual4): Sequential(\n",
       "      (0): ResBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): GLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): ResBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          (2): GLU()\n",
       "          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (img): GET_IMAGE_G(\n",
       "    (img): Sequential(\n",
       "      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### define models ###\n",
    "n_words = len(ixtoword)\n",
    "n_hidden = 256\n",
    "train_netE_path = '../DAMSMencoders/bird/text_encoder.pth'\n",
    "text_encoder = \\\n",
    "        RNN_ENCODER(n_words, nhidden=n_hidden)\n",
    "state_dict = \\\n",
    "        torch.load(train_netE_path, map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "print('Load text encoder from:', train_netE_path)\n",
    "text_encoder = text_encoder.cuda()\n",
    "text_encoder.eval()\n",
    "\n",
    "VGG = VGG16()\n",
    "print(\"Load the VGG model\")\n",
    "VGG.cuda()\n",
    "VGG.eval()\n",
    "\n",
    "\n",
    "netG = EncDecNet()\n",
    "model_dir = os.path.join(data_dir, 'output', netG_path, 'Model/netG_epoch_600.pth')\n",
    "state_dict = \\\n",
    "        torch.load(model_dir, map_location=lambda storage, loc: storage)\n",
    "netG.load_state_dict(state_dict)\n",
    "print('Load G from: ', model_dir)\n",
    "#netG = nn.DataParallel(netG, device_ids= self.gpus)\n",
    "netG.cuda()\n",
    "netG.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = Variable(torch.from_numpy(cap_array), volatile=True)\n",
    "cap_lens = Variable(torch.from_numpy(cap_lens), volatile=True)\n",
    "captions = captions.cuda()\n",
    "cap_lens = cap_lens.cuda()\n",
    "\n",
    "batch_size = captions.shape[0]\n",
    "nz = 100\n",
    "noise = Variable(torch.FloatTensor(batch_size, nz), volatile=True)\n",
    "noise = noise.cuda()\n",
    "\n",
    "hidden = text_encoder.init_hidden(batch_size)\n",
    "\n",
    "words_embs, sent_emb = text_encoder(captions, cap_lens, hidden)\n",
    "words_embs, sent_emb = words_embs.detach(), sent_emb.detach()\n",
    "\n",
    "mask = (captions == 0)\n",
    "\n",
    "noise.data.normal_(0, 1)\n",
    "\n",
    "from_imgs_cuda= [] \n",
    "to_imgs_cuda = []\n",
    "for i in range(len(from_imgs)):\n",
    "    from_imgs_cuda.append(from_imgs[i].cuda())    \n",
    "    to_imgs_cuda.append(to_imgs[i].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecNet' object has no attribute 'sent_dec2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-cf004524a701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnow_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_features_from\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msplit_rate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menc_features_to\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msplit_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnew_enc_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnow_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mfake_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_enc_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mfake_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Encdecmodel/code/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, sent_emb, words_embs, z_code, mask, enc_features)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;31m## 512 x 16 x 16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m#out_code = self.residual(out_code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m         \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_code1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Encdecmodel/code/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h_code, c_code, z_code, img, w_words_embs, mask, enc_features)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;31m#img2 = self.downimg2(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m#out_img = torch.cat((out_code, img2), dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mout_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_code3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_dec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_code2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_words_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m         \u001b[0mout_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mout_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2019.03/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecNet' object has no attribute 'sent_dec2'"
     ]
    }
   ],
   "source": [
    "enc_features_from = VGG(from_imgs_cuda[-1].unsqueeze(0))\n",
    "enc_features_to = VGG(to_imgs_cuda[-1].unsqueeze(0))\n",
    "split_list = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "fake_imgs = []\n",
    "for i in range(len(split_list)):\n",
    "    split_rate = split_list[i]\n",
    "    new_enc_features = []\n",
    "    for j in range(len(enc_features_from)):\n",
    "        now_enc = enc_features_from[j]*(1-split_rate) + enc_features_to[j]*split_rate\n",
    "        new_enc_features.append(now_enc)\n",
    "    fake_img, _,_ = netG(from_imgs[-1], sent_emb, words_embs, noise, mask, new_enc_features)\n",
    "    fake_imgs.append(fake_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
